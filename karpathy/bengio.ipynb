{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking away from Karapthy to try to implement the MLP paper myself. This is 2 decade old tech so hopefully it's not so difficult!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST, i'll try to re-create makemore-1's 1 layer NN in pytorch. I have no idea what I'm doing (reading docs!)\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((27, 27)))\n",
    "\n",
    "    def forward(self, x_enc, ys=None):\n",
    "        # xs --> input characters encoded with one_hot encoding\n",
    "        # ys --> output characters for every x (not encoded)\n",
    "        logits = x_enc @ self.W\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        # loss = mean negative log probability of the correct letter (ys) following the input\n",
    "        if self.training:\n",
    "            loss = -prob[torch.arange(x_enc.size(0)), ys].log().mean() \n",
    "            return logits, loss\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        x, y = stoi[ch1], stoi[ch2]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "xs, ys = torch.tensor(xs, device=device), torch.tensor(ys, device=device)\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4720, device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bigram()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "bigram = Bigram()\n",
    "bigram.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(bigram.parameters(), lr=50, weight_decay=0.0, momentum=0.0)\n",
    "\n",
    "loss = None\n",
    "for _ in range(100):\n",
    "    _, loss = bigram.forward(x_enc, ys)\n",
    "    bigram.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.data)\n",
    "bigram.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daima.\n",
      "sason.\n",
      "odorsh.\n",
      "jarascevonn.\n",
      "ehirulystz.\n",
      "ceyn.\n",
      "th.\n",
      "kyn.\n",
      "coromarikorimolirrasi.\n",
      "k.\n",
      "ayanedin.\n",
      "eillanyusieke.\n",
      "ka.\n",
      "horly.\n",
      "kyiavy.\n",
      "be.\n",
      "rilelleal.\n",
      "qhaigmodan.\n",
      "jariyliler.\n",
      "onalydan.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        input_char = F.one_hot(torch.tensor([ix], device=device), num_classes=27).float()\n",
    "        logits, _ = bigram.forward(input_char)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(prob, 1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # m --> numnber of features\n",
    "    # n --> context size\n",
    "    # h --> number of hidden units\n",
    "    def __init__(self, m, n, h):\n",
    "        super().__init__()\n",
    "        self.C = nn.Embedding(27, m)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(m * n, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, 27)\n",
    "        )\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, xs, ys=None):\n",
    "        features = self.C(xs).view((-1, self.m * self.n))\n",
    "        logits = self.mlp(features)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "\n",
    "        if self.training:\n",
    "            loss = -prob[torch.arange(xs.size(0)), ys].log().mean()\n",
    "            return logits, loss\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(3 * 2 * 4).reshape((3, 2, 4))\n",
    "print(t)\n",
    "t = t.reshape((3, 8))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs=tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        ...,\n",
      "        [26, 26, 25],\n",
      "        [26, 25, 26],\n",
      "        [25, 26, 24]], device='mps:0')\n",
      "ys=tensor([ 5, 13, 13,  ..., 26, 24,  0], device='mps:0')\n",
      "torch.Size([228146, 3])\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    chrs = ['.', '.', '.'] + list(word) + ['.']\n",
    "    for i in range(3, len(chrs)):\n",
    "        x = [stoi[c] for c in chrs[i-3:i]]\n",
    "        y = stoi[chrs[i]]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "xs = torch.tensor(xs).to(device=device)\n",
    "ys = torch.tensor(ys).to(device=device)\n",
    "print(f'{xs=}')\n",
    "print(f'{ys=}')\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2998, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW, back to Bengio et. al\n",
    "ix = xs.to('cpu')\n",
    "iy = ys.to('cpu')\n",
    "C = nn.Embedding(27, 10) # we are storing 10 \"features\" on each character\n",
    "hidden = nn.Linear(30, 60)\n",
    "tanh = nn.Tanh()\n",
    "output = nn.Linear(60, 27)\n",
    "features = C(ix).view(-1, 30)\n",
    "print(features.shape)\n",
    "hidden_applied = hidden(features)\n",
    "logits = output(tanh(hidden_applied))\n",
    "prob = F.softmax(logits, dim=1)\n",
    "loss = -prob[torch.arange(ix.size(0)), iy].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3, 11],\n",
       "        [ 9, 25,  1],\n",
       "        [ 0, 14,  9],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0, 10,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [12, 25, 14],\n",
       "        [ 4,  1, 19],\n",
       "        [ 1, 14,  7],\n",
       "        [12, 25, 14],\n",
       "        [ 0,  0,  2],\n",
       "        [ 5,  4, 18],\n",
       "        [ 9, 15, 14],\n",
       "        [ 0, 11,  5],\n",
       "        [18,  9, 11],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0, 16,  1],\n",
       "        [18,  9,  5],\n",
       "        [ 0,  0,  2],\n",
       "        [ 1,  8,  5],\n",
       "        [12, 15, 18],\n",
       "        [ 1,  1, 22],\n",
       "        [ 0,  0,  0],\n",
       "        [20, 20, 15],\n",
       "        [ 0,  0,  0],\n",
       "        [12, 25, 19],\n",
       "        [ 7,  1, 14],\n",
       "        [ 5,  5, 14],\n",
       "        [14, 14,  5],\n",
       "        [ 0,  0,  0]], device='mps:0')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[torch.randint(0, xs.size(0), (32,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t\t time: 0:00:00\t\t loss: 3.2337779998779297\n",
      "epoch: 10000\t\t time: 0:00:14\t\t loss: 2.2249069213867188\n",
      "epoch: 20000\t\t time: 0:00:28\t\t loss: 2.060516119003296\n",
      "epoch: 30000\t\t time: 0:00:42\t\t loss: 2.1540799140930176\n",
      "epoch: 40000\t\t time: 0:00:57\t\t loss: 2.179961919784546\n",
      "epoch: 50000\t\t time: 0:01:12\t\t loss: 1.9482200145721436\n",
      "epoch: 60000\t\t time: 0:01:27\t\t loss: 2.4588623046875\n",
      "epoch: 70000\t\t time: 0:01:43\t\t loss: 2.8087587356567383\n",
      "epoch: 80000\t\t time: 0:01:59\t\t loss: 2.0822691917419434\n",
      "epoch: 90000\t\t time: 0:02:14\t\t loss: 2.1595067977905273\n",
      "epoch: 100000\t\t time: 0:02:29\t\t loss: 1.9688297510147095\n",
      "epoch: 110000\t\t time: 0:02:43\t\t loss: 2.0223984718322754\n",
      "epoch: 120000\t\t time: 0:02:58\t\t loss: 2.496594190597534\n",
      "epoch: 130000\t\t time: 0:03:13\t\t loss: 2.1561193466186523\n",
      "epoch: 140000\t\t time: 0:03:29\t\t loss: 2.070702075958252\n",
      "epoch: 150000\t\t time: 0:03:43\t\t loss: 1.6359426975250244\n",
      "epoch: 160000\t\t time: 0:03:58\t\t loss: 2.6942901611328125\n",
      "epoch: 170000\t\t time: 0:04:14\t\t loss: 1.8253390789031982\n",
      "epoch: 180000\t\t time: 0:04:29\t\t loss: 1.9254202842712402\n",
      "epoch: 190000\t\t time: 0:04:44\t\t loss: 2.294869899749756\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[265], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m _, loss \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mforward(xs[batch], ys[batch])\n\u001b[1;32m     17\u001b[0m mlp\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Source/learn_ml/karpathy/venv/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Source/learn_ml/karpathy/venv/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Source/learn_ml/karpathy/venv/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "# initialize model\n",
    "mlp = MLP(10, 3, 200).to(device=device)\n",
    "\n",
    "# train\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "mlp.train()\n",
    "\n",
    "loss = None\n",
    "start_time = time.time()\n",
    "for epoch in range(1_200_000):\n",
    "    # minibatch\n",
    "    batch = torch.randint(0, xs.size(0), (32,))\n",
    "    _, loss = mlp.forward(xs[batch], ys[batch])\n",
    "\n",
    "    mlp.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if epoch % 10000 == 0:\n",
    "        time_elapsed = timedelta(seconds=int(time.time() - start_time))\n",
    "        print(f'epoch: {epoch}\\t\\t time: {time_elapsed}\\t\\t loss: {loss.item()}')\n",
    "print(loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "phlitr\n",
      "shiem\n",
      "shaylionam\n",
      "kotra\n",
      "jeilee\n",
      "alerinek\n",
      "osyne\n",
      "aryn\n",
      "alo\n",
      "togen\n",
      "arha\n",
      "eyuletieshit\n",
      "acyylyah\n",
      "jasdefrad\n",
      "abr\n",
      "thliria\n",
      "eygan\n",
      "hevraesannzer\n",
      "elsa\n",
      "aniveni\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "for _ in range(20):\n",
    "    out = ['.', '.', '.']\n",
    "    while True:\n",
    "        ix = 0\n",
    "        ctx = torch.tensor([stoi[i] for i in out[-3:]]).reshape(1, 3).to(device=device)\n",
    "        logits, _ = mlp.forward(ctx)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(prob, 1, replacement=True).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out)[3:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
