{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking away from Karapthy to try to implement the MLP paper myself. This is 2 decade old tech so hopefully it's not so difficult!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST, i'll try to re-create makemore-1's 1 layer NN in pytorch. I have no idea what I'm doing (reading docs!)\n",
    "class Bigram(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.randn((27, 27)))\n",
    "\n",
    "    def forward(self, x_enc, ys=None):\n",
    "        # xs --> input characters encoded with one_hot encoding\n",
    "        # ys --> output characters for every x (not encoded)\n",
    "        logits = x_enc @ self.W\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        # loss = mean negative log probability of the correct letter (ys) following the input\n",
    "        if self.training:\n",
    "            loss = -prob[torch.arange(x_enc.size(0)), ys].log().mean() \n",
    "            return logits, loss\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        x, y = stoi[ch1], stoi[ch2]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "xs, ys = torch.tensor(xs, device=device), torch.tensor(ys, device=device)\n",
    "x_enc = F.one_hot(xs, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4720, device='mps:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Bigram()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "bigram = Bigram()\n",
    "bigram.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(bigram.parameters(), lr=50, weight_decay=0.0, momentum=0.0)\n",
    "\n",
    "loss = None\n",
    "for _ in range(100):\n",
    "    _, loss = bigram.forward(x_enc, ys)\n",
    "    bigram.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.data)\n",
    "bigram.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daima.\n",
      "sason.\n",
      "odorsh.\n",
      "jarascevonn.\n",
      "ehirulystz.\n",
      "ceyn.\n",
      "th.\n",
      "kyn.\n",
      "coromarikorimolirrasi.\n",
      "k.\n",
      "ayanedin.\n",
      "eillanyusieke.\n",
      "ka.\n",
      "horly.\n",
      "kyiavy.\n",
      "be.\n",
      "rilelleal.\n",
      "qhaigmodan.\n",
      "jariyliler.\n",
      "onalydan.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "g = torch.Generator(device=device).manual_seed(2147483647)\n",
    "for _ in range(20):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        input_char = F.one_hot(torch.tensor([ix], device=device), num_classes=27).float()\n",
    "        logits, _ = bigram.forward(input_char)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(prob, 1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    # m --> numnber of features\n",
    "    # n --> context size\n",
    "    # h --> number of hidden units\n",
    "    def __init__(self, m, n, h):\n",
    "        super().__init__()\n",
    "        self.C = nn.Embedding(27, m)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(m * n, h),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(h, 27)\n",
    "        )\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, xs, ys=None):\n",
    "        features = self.C(xs).view((-1, self.m * self.n))\n",
    "        logits = self.mlp(features)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "\n",
    "        if self.training:\n",
    "            loss = -prob[torch.arange(xs.size(0)), ys].log().mean()\n",
    "            return logits, loss\n",
    "        return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7]],\n",
      "\n",
      "        [[ 8,  9, 10, 11],\n",
      "         [12, 13, 14, 15]],\n",
      "\n",
      "        [[16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.arange(3 * 2 * 4).reshape((3, 2, 4))\n",
    "print(t)\n",
    "t = t.reshape((3, 8))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs=tensor([[ 0,  0,  0],\n",
      "        [ 0,  0,  5],\n",
      "        [ 0,  5, 13],\n",
      "        ...,\n",
      "        [26, 26, 25],\n",
      "        [26, 25, 26],\n",
      "        [25, 26, 24]], device='mps:0')\n",
      "ys=tensor([ 5, 13, 13,  ..., 26, 24,  0], device='mps:0')\n",
      "torch.Size([228146, 3])\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "xs = []\n",
    "ys = []\n",
    "for word in words:\n",
    "    chrs = ['.', '.', '.'] + list(word) + ['.']\n",
    "    for i in range(3, len(chrs)):\n",
    "        x = [stoi[c] for c in chrs[i-3:i]]\n",
    "        y = stoi[chrs[i]]\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "xs = torch.tensor(xs).to(device=device)\n",
    "ys = torch.tensor(ys).to(device=device)\n",
    "print(f'{xs=}')\n",
    "print(f'{ys=}')\n",
    "print(xs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 30])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.2998, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW, back to Bengio et. al\n",
    "ix = xs.to('cpu')\n",
    "iy = ys.to('cpu')\n",
    "C = nn.Embedding(27, 10) # we are storing 10 \"features\" on each character\n",
    "hidden = nn.Linear(30, 60)\n",
    "tanh = nn.Tanh()\n",
    "output = nn.Linear(60, 27)\n",
    "features = C(ix).view(-1, 30)\n",
    "print(features.shape)\n",
    "hidden_applied = hidden(features)\n",
    "logits = output(tanh(hidden_applied))\n",
    "prob = F.softmax(logits, dim=1)\n",
    "loss = -prob[torch.arange(ix.size(0)), iy].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  3, 11],\n",
       "        [ 9, 25,  1],\n",
       "        [ 0, 14,  9],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0, 10,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [12, 25, 14],\n",
       "        [ 4,  1, 19],\n",
       "        [ 1, 14,  7],\n",
       "        [12, 25, 14],\n",
       "        [ 0,  0,  2],\n",
       "        [ 5,  4, 18],\n",
       "        [ 9, 15, 14],\n",
       "        [ 0, 11,  5],\n",
       "        [18,  9, 11],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0, 16,  1],\n",
       "        [18,  9,  5],\n",
       "        [ 0,  0,  2],\n",
       "        [ 1,  8,  5],\n",
       "        [12, 15, 18],\n",
       "        [ 1,  1, 22],\n",
       "        [ 0,  0,  0],\n",
       "        [20, 20, 15],\n",
       "        [ 0,  0,  0],\n",
       "        [12, 25, 19],\n",
       "        [ 7,  1, 14],\n",
       "        [ 5,  5, 14],\n",
       "        [14, 14,  5],\n",
       "        [ 0,  0,  0]], device='mps:0')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[torch.randint(0, xs.size(0), (32,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\t\t time: 0:00:00\t\t loss: 3.358635425567627\n",
      "epoch: 10000\t\t time: 0:00:15\t\t loss: 2.6853559017181396\n",
      "epoch: 20000\t\t time: 0:00:31\t\t loss: 2.1199324131011963\n",
      "epoch: 30000\t\t time: 0:00:49\t\t loss: 2.0650057792663574\n",
      "epoch: 40000\t\t time: 0:01:08\t\t loss: 2.405588150024414\n",
      "epoch: 50000\t\t time: 0:01:25\t\t loss: 2.2844316959381104\n",
      "epoch: 60000\t\t time: 0:01:41\t\t loss: 2.0930724143981934\n",
      "epoch: 70000\t\t time: 0:01:58\t\t loss: 2.0214450359344482\n",
      "epoch: 80000\t\t time: 0:02:15\t\t loss: 2.3207545280456543\n",
      "epoch: 90000\t\t time: 0:02:32\t\t loss: 2.2911877632141113\n",
      "epoch: 100000\t\t time: 0:02:48\t\t loss: 2.0628888607025146\n",
      "epoch: 110000\t\t time: 0:03:05\t\t loss: 1.9127233028411865\n",
      "epoch: 120000\t\t time: 0:03:23\t\t loss: 2.1825385093688965\n",
      "epoch: 130000\t\t time: 0:03:41\t\t loss: 2.047163963317871\n",
      "epoch: 140000\t\t time: 0:03:57\t\t loss: 2.1495680809020996\n",
      "epoch: 150000\t\t time: 0:04:13\t\t loss: 2.436847686767578\n",
      "epoch: 160000\t\t time: 0:04:29\t\t loss: 2.485835313796997\n",
      "epoch: 170000\t\t time: 0:04:45\t\t loss: 2.2899398803710938\n",
      "epoch: 180000\t\t time: 0:05:02\t\t loss: 1.9253876209259033\n",
      "epoch: 190000\t\t time: 0:05:18\t\t loss: 2.2388176918029785\n",
      "epoch: 200000\t\t time: 0:05:34\t\t loss: 2.8554935455322266\n",
      "epoch: 210000\t\t time: 0:05:50\t\t loss: 2.283596992492676\n",
      "epoch: 220000\t\t time: 0:06:06\t\t loss: 2.1044135093688965\n",
      "epoch: 230000\t\t time: 0:06:22\t\t loss: 1.955606460571289\n",
      "epoch: 240000\t\t time: 0:06:38\t\t loss: 2.031514883041382\n",
      "epoch: 250000\t\t time: 0:06:53\t\t loss: 2.1116132736206055\n",
      "epoch: 260000\t\t time: 0:07:09\t\t loss: 2.14786696434021\n",
      "epoch: 270000\t\t time: 0:07:25\t\t loss: 2.4327573776245117\n",
      "epoch: 280000\t\t time: 0:07:41\t\t loss: 2.771273374557495\n",
      "epoch: 290000\t\t time: 0:07:57\t\t loss: 1.9416170120239258\n",
      "epoch: 300000\t\t time: 0:08:13\t\t loss: 2.429616928100586\n",
      "epoch: 310000\t\t time: 0:08:29\t\t loss: 2.100282669067383\n",
      "epoch: 320000\t\t time: 0:08:44\t\t loss: 1.5954821109771729\n",
      "epoch: 330000\t\t time: 0:09:00\t\t loss: 2.2125630378723145\n",
      "epoch: 340000\t\t time: 0:09:16\t\t loss: 2.005467176437378\n",
      "epoch: 350000\t\t time: 0:09:32\t\t loss: 2.1850850582122803\n",
      "epoch: 360000\t\t time: 0:09:47\t\t loss: 2.3159515857696533\n",
      "epoch: 370000\t\t time: 0:10:03\t\t loss: 2.020390033721924\n",
      "epoch: 380000\t\t time: 0:10:19\t\t loss: 2.138524055480957\n",
      "epoch: 390000\t\t time: 0:10:34\t\t loss: 2.016512632369995\n",
      "epoch: 400000\t\t time: 0:10:50\t\t loss: 2.1789839267730713\n",
      "epoch: 410000\t\t time: 0:11:06\t\t loss: 2.2608165740966797\n",
      "epoch: 420000\t\t time: 0:11:22\t\t loss: 2.1573472023010254\n",
      "epoch: 430000\t\t time: 0:11:38\t\t loss: 2.718621253967285\n",
      "epoch: 440000\t\t time: 0:11:53\t\t loss: 2.5759799480438232\n",
      "epoch: 450000\t\t time: 0:12:09\t\t loss: 2.1992335319519043\n",
      "epoch: 460000\t\t time: 0:12:25\t\t loss: 2.5095953941345215\n",
      "epoch: 470000\t\t time: 0:12:40\t\t loss: 2.024669885635376\n",
      "epoch: 480000\t\t time: 0:12:56\t\t loss: 2.584463119506836\n",
      "epoch: 490000\t\t time: 0:13:11\t\t loss: 1.9398856163024902\n",
      "epoch: 500000\t\t time: 0:13:27\t\t loss: 1.9845269918441772\n",
      "epoch: 510000\t\t time: 0:13:43\t\t loss: 2.1247856616973877\n",
      "epoch: 520000\t\t time: 0:13:59\t\t loss: 2.287154197692871\n",
      "epoch: 530000\t\t time: 0:14:14\t\t loss: 2.2683746814727783\n",
      "epoch: 540000\t\t time: 0:14:30\t\t loss: 2.076885461807251\n",
      "epoch: 550000\t\t time: 0:14:47\t\t loss: 2.105194568634033\n",
      "epoch: 560000\t\t time: 0:15:03\t\t loss: 1.9873439073562622\n",
      "epoch: 570000\t\t time: 0:15:20\t\t loss: 2.274268627166748\n",
      "epoch: 580000\t\t time: 0:15:36\t\t loss: 2.340219020843506\n",
      "epoch: 590000\t\t time: 0:15:53\t\t loss: 2.3571062088012695\n",
      "epoch: 600000\t\t time: 0:16:10\t\t loss: 2.168955087661743\n",
      "epoch: 610000\t\t time: 0:16:27\t\t loss: 2.179347038269043\n",
      "epoch: 620000\t\t time: 0:16:44\t\t loss: 2.1650049686431885\n",
      "epoch: 630000\t\t time: 0:17:01\t\t loss: 2.398996353149414\n",
      "epoch: 640000\t\t time: 0:17:19\t\t loss: 2.395703077316284\n",
      "epoch: 650000\t\t time: 0:17:35\t\t loss: 2.18735933303833\n",
      "epoch: 660000\t\t time: 0:17:50\t\t loss: 2.385143280029297\n",
      "epoch: 670000\t\t time: 0:18:06\t\t loss: 2.1878316402435303\n",
      "epoch: 680000\t\t time: 0:18:22\t\t loss: 2.3632781505584717\n",
      "epoch: 690000\t\t time: 0:18:38\t\t loss: 2.362621307373047\n",
      "epoch: 700000\t\t time: 0:18:53\t\t loss: 1.9069302082061768\n",
      "epoch: 710000\t\t time: 0:19:09\t\t loss: 2.23461651802063\n",
      "epoch: 720000\t\t time: 0:19:25\t\t loss: 2.210367441177368\n",
      "epoch: 730000\t\t time: 0:19:41\t\t loss: 2.2206902503967285\n",
      "epoch: 740000\t\t time: 0:19:57\t\t loss: 2.482184886932373\n",
      "epoch: 750000\t\t time: 0:20:12\t\t loss: 2.0700254440307617\n",
      "epoch: 760000\t\t time: 0:20:28\t\t loss: 2.191160202026367\n",
      "epoch: 770000\t\t time: 0:20:44\t\t loss: 2.3361496925354004\n",
      "epoch: 780000\t\t time: 0:21:00\t\t loss: 1.8628904819488525\n",
      "epoch: 790000\t\t time: 0:21:16\t\t loss: 2.3865890502929688\n",
      "epoch: 800000\t\t time: 0:21:31\t\t loss: 2.376842975616455\n",
      "epoch: 810000\t\t time: 0:21:46\t\t loss: 2.1682944297790527\n",
      "epoch: 820000\t\t time: 0:22:02\t\t loss: 2.3432252407073975\n",
      "epoch: 830000\t\t time: 0:22:19\t\t loss: 2.2771213054656982\n",
      "epoch: 840000\t\t time: 0:22:43\t\t loss: 2.480558395385742\n",
      "epoch: 850000\t\t time: 0:23:05\t\t loss: 2.0704338550567627\n",
      "epoch: 860000\t\t time: 0:23:21\t\t loss: 2.367772102355957\n",
      "epoch: 870000\t\t time: 0:23:38\t\t loss: 2.3854422569274902\n",
      "epoch: 880000\t\t time: 0:23:55\t\t loss: 2.353837490081787\n",
      "epoch: 890000\t\t time: 0:24:12\t\t loss: 2.006131172180176\n",
      "epoch: 900000\t\t time: 0:24:29\t\t loss: 2.1953330039978027\n",
      "epoch: 910000\t\t time: 0:24:44\t\t loss: 2.315682888031006\n",
      "epoch: 920000\t\t time: 0:25:00\t\t loss: 2.320054769515991\n",
      "epoch: 930000\t\t time: 0:25:16\t\t loss: 2.310720920562744\n",
      "epoch: 940000\t\t time: 0:25:32\t\t loss: 2.0297770500183105\n",
      "epoch: 950000\t\t time: 0:25:48\t\t loss: 2.227609157562256\n",
      "epoch: 960000\t\t time: 0:26:04\t\t loss: 2.7699742317199707\n",
      "epoch: 970000\t\t time: 0:26:21\t\t loss: 2.5317344665527344\n",
      "epoch: 980000\t\t time: 0:26:37\t\t loss: 1.8401553630828857\n",
      "epoch: 990000\t\t time: 0:26:53\t\t loss: 1.8684924840927124\n",
      "epoch: 1000000\t\t time: 0:27:09\t\t loss: 2.142573356628418\n",
      "epoch: 1010000\t\t time: 0:27:25\t\t loss: 2.131462335586548\n",
      "epoch: 1020000\t\t time: 0:27:41\t\t loss: 1.8375993967056274\n",
      "epoch: 1030000\t\t time: 0:27:57\t\t loss: 2.0524868965148926\n",
      "epoch: 1040000\t\t time: 0:28:12\t\t loss: 2.3511452674865723\n",
      "epoch: 1050000\t\t time: 0:28:28\t\t loss: 2.2104177474975586\n",
      "epoch: 1060000\t\t time: 0:28:44\t\t loss: 2.3242995738983154\n",
      "epoch: 1070000\t\t time: 0:29:00\t\t loss: 1.9406452178955078\n",
      "epoch: 1080000\t\t time: 0:29:15\t\t loss: 2.3337442874908447\n",
      "epoch: 1090000\t\t time: 0:29:31\t\t loss: 2.5479235649108887\n",
      "epoch: 1100000\t\t time: 0:29:49\t\t loss: 2.1317007541656494\n",
      "epoch: 1110000\t\t time: 0:30:06\t\t loss: 2.1027607917785645\n",
      "epoch: 1120000\t\t time: 0:30:23\t\t loss: 2.0697708129882812\n",
      "epoch: 1130000\t\t time: 0:30:39\t\t loss: 2.1108343601226807\n",
      "epoch: 1140000\t\t time: 0:30:56\t\t loss: 2.0060172080993652\n",
      "epoch: 1150000\t\t time: 0:31:12\t\t loss: 2.4296960830688477\n",
      "epoch: 1160000\t\t time: 0:31:29\t\t loss: 1.9976484775543213\n",
      "epoch: 1170000\t\t time: 0:31:45\t\t loss: 2.3838367462158203\n",
      "epoch: 1180000\t\t time: 0:32:01\t\t loss: 2.2174744606018066\n",
      "epoch: 1190000\t\t time: 0:32:17\t\t loss: 2.1445696353912354\n",
      "tensor(2.2645, device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import timedelta\n",
    "# initialize model\n",
    "mlp = MLP(10, 3, 200).to(device=device)\n",
    "\n",
    "# train\n",
    "optimizer = torch.optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "mlp.train()\n",
    "\n",
    "loss = None\n",
    "start_time = time.time()\n",
    "for epoch in range(1_200_000):\n",
    "    # minibatch\n",
    "    batch = torch.randint(0, xs.size(0), (32,))\n",
    "    _, loss = mlp.forward(xs[batch], ys[batch])\n",
    "\n",
    "    mlp.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    if epoch % 10000 == 0:\n",
    "        time_elapsed = timedelta(seconds=int(time.time() - start_time))\n",
    "        print(f'epoch: {epoch}\\t\\t time: {time_elapsed}\\t\\t loss: {loss.item()}')\n",
    "print(loss.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ailax\n",
      "ryn\n",
      "yovy\n",
      "ann\n",
      "ber\n",
      "maharid\n",
      "mariki\n",
      "lija\n",
      "avirtevilxyn\n",
      "lon\n",
      "tio\n",
      "iyah\n",
      "kan\n",
      "mivionen\n",
      "zim\n",
      "jakanna\n",
      "cha\n",
      "dalya\n",
      "adamari\n",
      "masiem\n"
     ]
    }
   ],
   "source": [
    "mlp.eval()\n",
    "for _ in range(20):\n",
    "    out = ['.', '.', '.']\n",
    "    while True:\n",
    "        ix = 0\n",
    "        ctx = torch.tensor([stoi[i] for i in out[-3:]]).reshape(1, 3).to(device=device)\n",
    "        logits, _ = mlp.forward(ctx)\n",
    "        prob = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(prob, 1, replacement=True).item()\n",
    "        if ix == 0:\n",
    "            break\n",
    "        out.append(itos[ix])\n",
    "    print(''.join(out)[3:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
